{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supabase Data Migration\n",
    "\n",
    "Use this notebook to upload your local model and data artifacts to **Supabase Storage** so your API can download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Client configured for: https://nbjyhxzkwsxprivrteaw.supabase.co/\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# from pathlib import Path\n",
    "# from dotenv import load_dotenv\n",
    "# from supabase import create_client, Client\n",
    "\n",
    "# # 1. Load Environment Variables (SUPABASE_URL, SUPABASE_KEY)\n",
    "# load_dotenv(override=True)\n",
    "\n",
    "# url = os.environ.get(\"SUPABASE_URL\")\n",
    "# key = os.environ.get(\"SUPABASE_KEY\")\n",
    "\n",
    "# if not url or not key:\n",
    "#     raise ValueError(\"‚ùå Missing SUPABASE_URL or SUPABASE_KEY in .env file\")\n",
    "\n",
    "# print(f\"‚úÖ Credentials found for: {url}\")\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# 1. Load Environment Variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Use SECRET_KEY for administrative tasks like uploading\n",
    "url = os.environ.get(\"SUPABASE_URL\", \"\").strip().replace('\"', '').replace(\"'\", \"\")\n",
    "key = os.environ.get(\"SUPABASE_SECRET_KEY\", \"\").strip().replace('\"', '').replace(\"'\", \"\")\n",
    "\n",
    "# Force trailing slash and remove any double slashes that might occur\n",
    "url = url.rstrip(\"/\") + \"/\"\n",
    "\n",
    "if not url or not key:\n",
    "    raise ValueError(\"‚ùå Missing credentials in .env file. Ensure SUPABASE_URL and SUPABASE_SECRET_KEY are set.\")\n",
    "\n",
    "print(f\"‚úÖ Client configured for: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storage endpoint URL should have a trailing slash.\n",
      "‚úÖ Bucket 'housing-data' exists.\n"
     ]
    }
   ],
   "source": [
    "# 2. Initialize Supabase Client\n",
    "supabase: Client = create_client(url, key)\n",
    "bucket_name = \"housing-data\"\n",
    "\n",
    "# Check if bucket exists, or create it (requires appropriate permissions)\n",
    "try:\n",
    "    buckets = supabase.storage.list_buckets()\n",
    "    bucket_names = [b.name for b in buckets]\n",
    "    if bucket_name not in bucket_names:\n",
    "        print(f\"‚ö†Ô∏è Bucket '{bucket_name}' not found. Please create it in the Supabase Dashboard -> Storage -> New Bucket.\")\n",
    "        print(\"   Make sure to set it as PUBLIC if you want easy public access, or PRIVATE for authenticated download (our API uses authenticated).\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Bucket '{bucket_name}' exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error connecting to Supabase: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Upload Functions\n",
    "\n",
    "def upload_file(local_path: str, remote_path: str):\n",
    "    path_obj = Path(local_path)\n",
    "    if not path_obj.exists():\n",
    "        print(f\"‚ùå File not found: {local_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"üì§ Uploading {local_path} -> {bucket_name}/{remote_path} ...\")\n",
    "    \n",
    "    try:\n",
    "        with open(local_path, \"rb\") as f:\n",
    "            supabase.storage.from_(bucket_name).upload(\n",
    "                file=f,\n",
    "                path=remote_path,\n",
    "                file_options={\"cache-control\": \"3600\", \"upsert\": \"true\"}\n",
    "            )\n",
    "        print(\"   ‚úÖ Upload successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Upload failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Uploading ../models/xgb_best_model.pkl -> housing-data/models/xgb_best_model.pkl ...\n",
      "   ‚úÖ Upload successful!\n",
      "üì§ Uploading ../data/processed/feature_engineered_train.csv -> housing-data/data/processed/feature_engineered_train.csv ...\n",
      "   ‚ùå Upload failed: {'statusCode': 413, 'error': Payload too large, 'message': The object exceeded the maximum allowed size}\n"
     ]
    }
   ],
   "source": [
    "# 4. Upload Code\n",
    "\n",
    "# Upload Model\n",
    "upload_file(\n",
    "    local_path=\"../models/xgb_best_model.pkl\", \n",
    "    remote_path=\"models/xgb_best_model.pkl\"\n",
    ")\n",
    "\n",
    "# Upload Training Data (for features check)\n",
    "upload_file(\n",
    "    local_path=\"../data/processed/feature_engineered_train.csv\", \n",
    "    remote_path=\"data/processed/feature_engineered_train.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÇÔ∏è Creating tiny schema file...\n",
      "üì§ Uploading ../data/processed/train_schema_only.csv -> housing-data/data/processed/feature_engineered_train.csv ...\n",
      "   ‚úÖ Upload successful!\n"
     ]
    }
   ],
   "source": [
    "# 5. Create and Upload a \"Tiny\" version of the training data (just for column names)\n",
    "# This avoids the 181MB 413 error!\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "local_full_path = \"../data/processed/feature_engineered_train.csv\"\n",
    "local_tiny_path = \"../data/processed/train_schema_only.csv\"\n",
    "remote_path = \"data/processed/feature_engineered_train.csv\"\n",
    "\n",
    "if os.path.exists(local_full_path):\n",
    "    print(\"‚úÇÔ∏è Creating tiny schema file...\")\n",
    "    # Read only the first 5 rows\n",
    "    df_tiny = pd.read_csv(local_full_path, nrows=5)\n",
    "    # Save it locally\n",
    "    df_tiny.to_csv(local_tiny_path, index=False)\n",
    "    \n",
    "    # Upload this tiny file to the same location the API expects\n",
    "    upload_file(local_tiny_path, remote_path)\n",
    "else:\n",
    "    print(\"‚ùå Error: Original training file not found to create schema.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
